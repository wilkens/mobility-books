{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67de4a35-c08c-4dae-b74c-f8c5ca4b6733",
   "metadata": {},
   "source": [
    "# Mobility distance calculations (Matt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768b528c-c06c-4d1a-ba6a-1ca94d500eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "from   ast import literal_eval\n",
    "from   collections import Counter, defaultdict\n",
    "from   geopy import distance\n",
    "from   itertools import pairwise\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from   sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import string\n",
    "from   unidecode import unidecode\n",
    "\n",
    "data_dir = os.path.join('..', 'data')\n",
    "derived_dir = os.path.join(data_dir, 'derived')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654d317-01c5-47ba-b4e4-5cf4399f0e12",
   "metadata": {},
   "source": [
    "## Load derived data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c137454e-a1a9-4b29-b3a0-9817fab9651d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def string_to_list(x):\n",
    "    lst = literal_eval(x.replace(', nan', \"', ZZZZ'\").replace('[nan, ', \"'['\").replace(', nan]', \"']'\"))\n",
    "    return [i for i in lst if i != 'ZZZZ']\n",
    "\n",
    "conlit = pd.read_csv(\n",
    "    os.path.join(derived_dir, 'CONLIT_CharData_AP_6.csv.gz'), \n",
    "    index_col='book_id',\n",
    "    converters={\n",
    "        'gpe_places': string_to_list,\n",
    "        'nongpe_places': string_to_list,\n",
    "        'all_places': string_to_list,\n",
    "        'gpe_sequences': string_to_list\n",
    "    },\n",
    ")\n",
    "conlit = conlit.fillna('')#.drop(columns='X')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2009fc1e-4663-4edc-bf48-c3202138f4d3",
   "metadata": {},
   "source": [
    "## CONLIT GPE distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e1c48c9-1b07-409a-a526-19edc2a12d89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('New York', 6683),\n",
       " ('London', 4367),\n",
       " ('Paris', 3673),\n",
       " ('America', 3477),\n",
       " ('Washington', 2412),\n",
       " ('England', 2087),\n",
       " ('California', 2064),\n",
       " ('Chicago', 2000),\n",
       " ('Boston', 1624),\n",
       " ('France', 1560),\n",
       " ('Rome', 1304),\n",
       " ('San Francisco', 1247),\n",
       " ('Los Angeles', 1247),\n",
       " ('Texas', 1047),\n",
       " ('New York City', 1046),\n",
       " ('Europe', 952),\n",
       " ('Berlin', 938),\n",
       " ('Manhattan', 908),\n",
       " ('Philadelphia', 862),\n",
       " ('Virginia', 854)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpe_counts = Counter()\n",
    "for i in conlit.gpe_places:\n",
    "    gpe_counts.update(i)\n",
    "gpe_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e769576-ae28-4753-960a-bb9942c15ace",
   "metadata": {},
   "source": [
    "### Wilkens geo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3e8c66-f479-4722-af6e-906b7c4b2e95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# regularize data to wilkens geo format\n",
    "punctuation_to_space = str.maketrans({key:' ' for key in string.punctuation})\n",
    "\n",
    "def regularize_string(place_string):\n",
    "    return(unidecode(' '.join(place_string.translate(punctuation_to_space).lower().split())))\n",
    "\n",
    "wi = pd.read_csv(\n",
    "    os.path.join('..', '..', 'toponyms', 'geo.tsv.gz'),\n",
    "    sep='\\t',\n",
    "    low_memory=False,\n",
    ")\n",
    "wi = wi.loc[wi.lang=='en']\n",
    "wi.set_index('text_string', inplace=True)\n",
    "\n",
    "# hand review data\n",
    "hand = pd.read_csv(\n",
    "    os.path.join('..', '..', 'toponyms', 'us_handreview.tsv'),\n",
    "    sep='\\t',\n",
    "    index_col='text_string'\n",
    ")\n",
    "\n",
    "# restore some items from C19 hand review\n",
    "hand.loc[\n",
    "    [\n",
    "        'hollywood', \n",
    "        'dallas', \n",
    "        'florence', \n",
    "        'kingston',\n",
    "        'berkeley', \n",
    "        'queens', \n",
    "        'phoenix', \n",
    "        'woodstock', \n",
    "        'surrey',\n",
    "        'orlando'\n",
    "    ], \n",
    "    'ignore'\n",
    "] = 0\n",
    "\n",
    "# improve alises\n",
    "hand.loc['kingston', 'alias_to'] = 'kingston jamaica'\n",
    "hand.loc['baltic', 'alias_to'] = 'baltic sea'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a105d1-c1c6-46b8-bb70-77f7b32c09c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop ignored places\n",
    "wi = wi.drop(hand.loc[hand.ignore==1].index, errors='ignore')\n",
    "\n",
    "# alias places\n",
    "for original_place, alias_to in hand.loc[(~hand.alias_to.isna()) & (hand.ignore==0) & (hand.alias_to.isin(wi.index)), 'alias_to'].items():\n",
    "    wi.loc[original_place] = wi.loc[alias_to]\n",
    "\n",
    "# drop unused places\n",
    "wi_gpes = Counter()\n",
    "for sequence in conlit.gpe_sequences:\n",
    "    wi_gpes.update([regularize_string(i) for i in sequence])\n",
    "wi.drop(wi.loc[~wi.index.isin(wi_gpes)].index, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967cdeed-45eb-48e7-8a1b-f73de5835064",
   "metadata": {},
   "source": [
    "### Sequence distances\n",
    "\n",
    "For each volume sequence, look up each place, get lat/lon, calculate distance from previous place, sum over sequential path.\n",
    "\n",
    "If `source='wilkens'`, also perform fancier location aliasing, ignore known-bad places, and zero out sequence steps that move between admin levels within the same admin entity (e.g., `Boston -> United States` or `UK -> England`). The last step has room for improvement: we don't deal with `admin_2` level and below (the distances involved are small), nor with continents (Google data doesn't place countries in continents, plus, I think there's a meaningful sense in which there's distance between, e.g., `Paris` and `Europe` in a way that there isn't between `Boston` and `USA`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ace81b-4551-44ec-bebe-92ba10b3d92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_lat_lon(place_string, geo_data=wi, extended=False):\n",
    "    '''Assumes lookup string has been regularized if necessary'''\n",
    "    try:\n",
    "        if extended:\n",
    "            return(geo_data.loc[place_string, ['lat', 'lon', 'country_short', 'admin_1_std', 'location_type']])\n",
    "        else:\n",
    "            return(geo_data.loc[place_string, ['lat', 'lon']])\n",
    "    except (ValueError, KeyError):\n",
    "        return(None)\n",
    "\n",
    "def hop_distance(location1, location2, geo_data=wi, return_zero_dist=False):\n",
    "    '''\n",
    "    Takes two location strings, returns fancy distance between them in miles.\n",
    "    return_zero_dist: if True, return zero distances where we would otherwise return None\n",
    "    '''\n",
    "    dist = None\n",
    "    if location1 in distances and location2 in distances[location1]:\n",
    "        return distances[location1][location2]\n",
    "    else:\n",
    "        loc1 = get_lat_lon(location1, geo_data, extended=True)\n",
    "        loc2 = get_lat_lon(location2, geo_data, extended=True)\n",
    "        if loc1 is None or loc2 is None: pass # should never happen, but check\n",
    "        elif loc1.equals(loc2): pass # ignore identical places, even if called different names\n",
    "        # eliminate place -> higher-order place in same admin area\n",
    "        elif (loc1.location_type=='country' or loc2.location_type=='country') and \\\n",
    "           (loc2.country_short==loc1.country_short): pass\n",
    "        elif (loc1.location_type=='administrative_area_level_1' or \\\n",
    "              loc2.location_type=='administrative_area_level_1') and \\\n",
    "             (loc2.country_short==loc1.country_short and \\\n",
    "              loc2.admin_1_std==loc1.admin_1_std): pass\n",
    "        else:\n",
    "            loc1 = loc1[['lat', 'lon']]\n",
    "            loc2 = loc2[['lat', 'lon']]   \n",
    "            dist = distance.distance(loc1, loc2).miles\n",
    "            distances[location1][location2] = dist\n",
    "            distances[location2][location1] = dist\n",
    "    if return_zero_dist and dist is None:\n",
    "        dist = 0.0\n",
    "    return(dist)\n",
    "\n",
    "def sequence_distance(sequence, geo_data=wi):\n",
    "    # set data, regularize strings, and remove unknown locations\n",
    "    seq = [regularize_string(i) for i in sequence if regularize_string(i) in geo_data.index]\n",
    "    total_distance = 0.0\n",
    "    hop_distances = []\n",
    "    # calculate distance over pairwise hops\n",
    "    for location1, location2 in pairwise(seq):\n",
    "        dist = hop_distance(location1, location2, geo_data)\n",
    "        if dist != None:\n",
    "            total_distance += dist\n",
    "            hop_distances.append(dist)\n",
    "    # calculate start-finish distance\n",
    "    start_finish_miles = None\n",
    "    start_finish_Z = None\n",
    "    if len(seq) < 2:\n",
    "        pass\n",
    "    else:\n",
    "        start_finish_miles = hop_distance(seq[0], seq[-1], geo_data, return_zero_dist=True)\n",
    "    if len(hop_distances) > 1:\n",
    "        hop_std = np.std(hop_distances)\n",
    "        hop_mean = np.mean(hop_distances)\n",
    "        if hop_std > 0.01:\n",
    "            start_finish_Z = (start_finish_miles - hop_mean)/hop_std \n",
    "    return(total_distance, start_finish_miles, start_finish_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc90c2c1-95e4-4fba-af75-f2cc532a6877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1209.546836009187, 1209.546836009187, None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = defaultdict(lambda: defaultdict(float))\n",
    "sequence_distance(['baltic', 'connecticut'], geo_data=wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc92859-a3ca-4e61-a4b3-f7a69680f570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang                                                                en\n",
       "occurs                                                        262356.0\n",
       "place_id                                   ChIJyfT6OvJHiYcRTV5-9FrmUIA\n",
       "formatted_address    Baltic City Hall, 130 Street Olaf Ave, Baltic,...\n",
       "location_type                                  local_government_office\n",
       "country_long                                             United States\n",
       "country_short                                                       US\n",
       "admin_1_long                                              South Dakota\n",
       "admin_1_short                                                       SD\n",
       "admin_1_std                                                         SD\n",
       "admin_2                                               Minnehaha County\n",
       "admin_3                                                            NaN\n",
       "admin_4                                                            NaN\n",
       "admin_5                                                            NaN\n",
       "locality                                                        Baltic\n",
       "sublocality_1                                                      NaN\n",
       "sublocality_2                                                      NaN\n",
       "sublocality_3                                                      NaN\n",
       "neighborhood                                                       NaN\n",
       "premise                                                            NaN\n",
       "subpremise                                                         NaN\n",
       "street_number                                                      130\n",
       "street_address                                                     NaN\n",
       "route                                               Street Olaf Avenue\n",
       "post_code                                                        57003\n",
       "natural_feature                                                    NaN\n",
       "point_of_interest                                                  NaN\n",
       "colloquial_area                                                    NaN\n",
       "continent                                                          NaN\n",
       "other                                                              NaN\n",
       "lat                                                          43.760276\n",
       "lon                                                         -96.740097\n",
       "ignore                                                             NaN\n",
       "Name: baltic, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wi.loc['baltic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d525ac-83e6-496a-8e07-79812d195d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.3 s, sys: 7.11 ms, total: 26.3 s\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "distances = defaultdict(lambda: defaultdict(float))\n",
    "wi_distances = conlit['gpe_sequences'].apply(sequence_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bef9c-23e8-4283-9d98-fd22c9987c5a",
   "metadata": {},
   "source": [
    "### Save new distances to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f08c90-d7c8-4ec6-8506-2e99f80aa29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conlit['dist_miles'], conlit['Start_Finish_Miles'], conlit['Start_Finish_Z'] = zip(*wi_distances)\n",
    "conlit.to_csv(os.path.join(derived_dir, 'CONLIT_CharData_AP_MW_6.csv.gz'))\n",
    "conlit[['dist_miles', 'Start_Finish_Miles', 'Start_Finish_Z']].to_csv(os.path.join(derived_dir, 'CONLIT_CharData_distances_6.csv.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5892a-7731-4ad1-b615-5fd406e9cb85",
   "metadata": {},
   "source": [
    "## EARLY GPE distances\n",
    "\n",
    "### Prepare early file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5860a096-f9c6-424a-b015-7b060931ecd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_hoplist(file_path, label='gpe'):\n",
    "    df = pd.read_json(\n",
    "        file_path,\n",
    "        lines=True\n",
    "    ).explode('chars')\n",
    "    \n",
    "    book_ids = []\n",
    "    char_ids = []\n",
    "    gpe_lists = []\n",
    "    seq_lists = []\n",
    "    for _, row in df.iterrows():\n",
    "        book_id = row.book_id\n",
    "        d = row.loc['chars']\n",
    "        char_id = d['char_id']\n",
    "        seq = d['sequence']\n",
    "        seq_list = [place_dict['place'] for place_dict in seq]\n",
    "        gpe_list = []\n",
    "        for place_dict in seq:\n",
    "            for i in range(place_dict['count']):\n",
    "                gpe_list.append(place_dict['place'])\n",
    "        seq_lists.append(seq_list)\n",
    "        gpe_lists.append(gpe_list)\n",
    "        book_ids.append(book_id)\n",
    "        char_ids.append(char_id)\n",
    "\n",
    "    result = pd.DataFrame(\n",
    "        {\n",
    "            'book_id':book_ids,\n",
    "            'char_id':char_ids,\n",
    "            f'{label}_places':gpe_lists,\n",
    "            f'{label}_sequences':seq_lists\n",
    "        }\n",
    "    )\n",
    "    return(result)\n",
    "\n",
    "# read hoplists\n",
    "early_gpes = read_hoplist(os.path.join(derived_dir, 'mb.hoplist.gpe.all.jsonl.bz2'), label='gpe')\n",
    "early_nongpes = read_hoplist(os.path.join(derived_dir, 'mb.hoplist.non_gpe.all.jsonl.bz2'), label='nongpe')\n",
    "early_allplaces = read_hoplist(os.path.join(derived_dir, 'mb.hoplist.all.all.jsonl.bz2'), label='all')\n",
    "\n",
    "# read base data\n",
    "base_early = pd.read_csv(\n",
    "    os.path.join(derived_dir, 'inf_gender.1.tsv.gz'),\n",
    "    sep='\\t',\n",
    ").drop(columns=['prob'])\n",
    "\n",
    "# restrict to protagonists\n",
    "early = base_early.merge(\n",
    "    early_gpes, how='left', on=['book_id', 'char_id']\n",
    ").merge(\n",
    "    early_nongpes, how='left', on=['book_id', 'char_id']\n",
    ").merge(\n",
    "    early_allplaces, how='left', on=['book_id', 'char_id']\n",
    ")\n",
    "\n",
    "# reference format of original data for CONLIT\n",
    "ref = pd.read_csv(\n",
    "    os.path.join(derived_dir, 'book_char_mobility.tsv.bz2'),\n",
    "    sep='\\t'\n",
    ")\n",
    "\n",
    "# calculate simple derived columns\n",
    "early['num_gpe_places'] = early['gpe_places'].apply(lambda x: len(set(x)))\n",
    "early['num_nongpe_places'] = early['nongpe_places'].apply(lambda x: len(set(x)))\n",
    "early['num_all_places'] = early['all_places'].apply(lambda x: len(set(x)))\n",
    "early['char_rank'] = 1.0\n",
    "\n",
    "# add token count\n",
    "early_token_counts = pd.read_csv(\n",
    "    os.path.join(derived_dir, 'mb.book_lengths.tsv'),\n",
    "    sep='\\t',\n",
    "    skiprows=1,\n",
    "    names=['book_id', 'Tokens']\n",
    ")\n",
    "early = early.merge(early_token_counts, how='left', on=['book_id'])\n",
    "\n",
    "# add empty distance column and reorder columns to match reference\n",
    "early['dist_miles'] = 0.0\n",
    "proper_cols = [col for col in ref.columns.drop('num_words')]\n",
    "proper_cols.append('Tokens')\n",
    "early = early[proper_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369353d4-2c65-409e-96cc-cf3a04f81c90",
   "metadata": {},
   "source": [
    "### Calculate distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e66b180-be45-41b4-8668-19f606eeb2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reread geo data to retain relevant locations, etc.\n",
    "wi = pd.read_csv(\n",
    "    os.path.join('..', '..', 'toponyms', 'geo.tsv.gz'),\n",
    "    sep='\\t',\n",
    "    low_memory=False,\n",
    ")\n",
    "wi = wi.loc[wi.lang=='en']\n",
    "wi.set_index('text_string', inplace=True)\n",
    "\n",
    "# hand review data\n",
    "hand = pd.read_csv(\n",
    "    os.path.join('..', '..', 'toponyms', 'us_handreview.tsv'),\n",
    "    sep='\\t',\n",
    "    index_col='text_string'\n",
    ")\n",
    "\n",
    "# restore some items from C19 hand review\n",
    "hand.loc[\n",
    "    [\n",
    "        'hollywood', \n",
    "        'dallas', \n",
    "        'florence', \n",
    "        'kingston',\n",
    "        'berkeley', \n",
    "        'queens', \n",
    "        'phoenix', \n",
    "        'woodstock', \n",
    "        'surrey',\n",
    "        'orlando'\n",
    "    ], \n",
    "    'ignore'\n",
    "] = 0\n",
    "\n",
    "# improve alises\n",
    "hand.loc['kingston', 'alias_to'] = 'kingston jamaica'\n",
    "\n",
    "# drop ignored places\n",
    "wi = wi.drop(hand.loc[hand.ignore==1].index, errors='ignore')\n",
    "\n",
    "# alias places\n",
    "for original_place, alias_to in hand.loc[(~hand.alias_to.isna()) & (hand.ignore==0) & (hand.alias_to.isin(wi.index)), 'alias_to'].items():\n",
    "    wi.loc[original_place] = wi.loc[alias_to]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6191cb89-5e9a-4673-920a-24a53cc8e74b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop unused places\n",
    "wi_gpes = Counter()\n",
    "for sequence in early.gpe_sequences:\n",
    "    wi_gpes.update([regularize_string(i) for i in sequence])\n",
    "wi.drop(wi.loc[~wi.index.isin(wi_gpes)].index, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "609a7f63-9781-4b60-baf1-49944a845140",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.5 s, sys: 8.1 ms, total: 32.5 s\n",
      "Wall time: 32.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "distances = defaultdict(lambda: defaultdict(float))\n",
    "early_distances = early['gpe_sequences'].apply(sequence_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e60fc2-f6d5-47ba-b755-6981de7c80d9",
   "metadata": {},
   "source": [
    "### Save new distances to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e73bbae-c375-4e7b-9850-b4f1ef7024b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early['dist_miles'], early['Start_Finish_Miles'], early['Start_Finish_Z'] = zip(*early_distances)\n",
    "early.set_index('book_id', inplace=True)\n",
    "early[['dist_miles', 'Start_Finish_Miles', 'Start_Finish_Z']].to_csv(os.path.join(derived_dir, 'EARLY_CharData_distances_2.csv.gz'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
